{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../datasets/PARKINSON_HW'\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../datasets/PARKINSON_HW\n",
    "rm -f PARKINSON_HW.zip\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00395/PARKINSON_HW.zip\n",
    "unzip PARKINSON_HW.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstfile = os.path.join(dataset_dir,\"hw_dataset\",\"control\", 'C_0001.txt')\n",
    "secondfile = os.path.join(dataset_dir,\"hw_dataset\",\"control\", 'C_0002.txt')\n",
    "thirdfile = os.path.join(dataset_dir,\"hw_dataset\",\"control\", 'C_0003.txt')\n",
    "forthfile = os.path.join(dataset_dir,\"hw_dataset\",\"control\", 'C_0004.txt')\n",
    "\n",
    "combinefile = os.path.join(dataset_dir,\"hw_dataset\",\"control\", 'C_join.txt')\n",
    "\n",
    "data = data2 = data3 = data4 = \"\" \n",
    "\n",
    "# Reading data from file1 \n",
    "with open(firstfile) as fp: \n",
    "\tdata = fp.read() \n",
    "\n",
    "# Reading data from file2 \n",
    "with open(secondfile) as fp: \n",
    "\tdata2 = fp.read() \n",
    "    \n",
    "# Reading data from file3\n",
    "with open(thirdfile) as fp: \n",
    "\tdata3 = fp.read() \n",
    "    \n",
    "with open(forthfile) as fp: \n",
    "\tdata4 = fp.read() \n",
    "    \n",
    "    \n",
    "# Merging 2 files \n",
    "# To add the data of file2 \n",
    "# from next line \n",
    "#data += \"\\n\"\n",
    "data += data2 \n",
    "data += data3\n",
    "data += data4\n",
    "with open (combinefile, 'w') as fp: \n",
    "\tfp.write(data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = \"dataset_dir\\\\hw_datset\\\\parkinson\"\n",
    "#print (data_folder)\n",
    "def read_df(filename):\n",
    "    return pd.read_csv(os.path.join(dataset_dir, \"hw_dataset\",\"parkinson\",filename), dtype=str)\n",
    "\n",
    "dataframes = read_df('P_02100002.txt')  #for filename in ['P_02100001.txt', 'P_02100002.txt'] ]\n",
    "dataset_files = os.path.join(dataset_dir,\"hw_dataset\",\"control\", 'C_join.txt')\n",
    "#df = pd.concat(dataframes)\n",
    "#.reset_index(drop=True)\n",
    "#print (dataframes)\n",
    "columns = ['X','Y', 'Z', 'Pressure', 'GripAngle','timestamp','class']\n",
    "#[ 'f{}'.format(i) for i in range(0, 5) ] + [ 'class' ]\n",
    "\n",
    "def load_df(file):\n",
    "    rows = []\n",
    "    with open(file) as f:\n",
    "        for line in f.readlines():\n",
    "            # Remove \\n\n",
    "            #print (line)\n",
    "           # print (\"hello\")\n",
    "           # line = line[:-2]\n",
    "            values = line.split(';')\n",
    "            #print (values)\n",
    "            class_value = values[6]\n",
    "            #print(class_value)\n",
    "            c_v = class_value.split('\\n')[0]\n",
    "            #print (c_v)\n",
    "            feature_values = [values[v] for v in range (6) ]   #[ v.split(':')[1] for v in values[1:] ]\n",
    "            #print (feature_values)\n",
    "            rows.append(feature_values + [c_v])\n",
    "            \n",
    "    #print (rows)\n",
    "    return pd.DataFrame(rows, columns=columns, dtype=str)\n",
    "\n",
    "df = load_df(dataset_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = df.shape[0]\n",
    "shuffled_df = df.sample(frac=1, replace=False, random_state=row_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df.to_csv(os.path.join(dataset_dir, 'control-shuffled.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
